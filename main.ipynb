{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    \"\"\"Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\"\"\"\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from n_body_system.dataset_nbody import NBodyDataset\n",
    "from dataset import NBodyDataset\n",
    "# from n_body_system.model import EGNN_vel\n",
    "from EGNN import EGNN_vel\n",
    "# from functions_utils import unsorted_segment_mean\n",
    "import os\n",
    "from torch import nn, optim\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'egnn_task2'\n",
    "batch_size = 100\n",
    "epochs = 1000\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "log_interval = 1\n",
    "test_interval = 5\n",
    "outf = './logs'\n",
    "lr = 5e-4\n",
    "nf = 64\n",
    "attention = 0\n",
    "n_layers = 4\n",
    "degree = 2\n",
    "max_training_samples = 3000\n",
    "dataset = \"nbody_small\"\n",
    "sweep_training = 0\n",
    "time_exp = 0\n",
    "weight_decay = 1e-12\n",
    "div = 1\n",
    "norm_diff = False\n",
    "tanh = False\n",
    "\n",
    "time_exp_dic = {'time': 0, 'counter': 0}\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log directory\n",
    "try:\n",
    "    os.makedirs(outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(outf + \"/\" + model_name)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "loss_mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity_attr(loc, vel, rows, cols):\n",
    "\n",
    "    diff = loc[cols] - loc[rows]\n",
    "    norm = torch.norm(diff, p=2, dim=1).unsqueeze(1)\n",
    "    u = diff/norm\n",
    "    va, vb = vel[rows] * u, vel[cols] * u\n",
    "    va, vb = torch.sum(va, dim=1).unsqueeze(1), torch.sum(vb, dim=1).unsqueeze(1)\n",
    "    return va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Datasets and loaders\n",
    "# dataset_train = NBodyDataset(partition='train', dataset_name=dataset,\n",
    "#                                  max_samples=max_training_samples)\n",
    "# loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# dataset_val = NBodyDataset(partition='val', dataset_name=\"nbody_small\")\n",
    "# loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# dataset_test = NBodyDataset(partition='test', dataset_name=\"nbody_small\")\n",
    "# loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "paths = glob.glob(\"data/task1_2/train/*.npz\")\n",
    "\n",
    "train_paths = paths[:int(len(paths)*0.8)]\n",
    "val_paths = paths[int(len(paths)*0.8):]\n",
    "\n",
    "dataset_train = NBodyDataset(train_paths)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, drop_last=True) # TODO: shuffle=True\n",
    "\n",
    "dataset_val = NBodyDataset(val_paths)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataset_test = NBodyDataset(val_paths)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3, 1], edge_index=[2, 6], edge_attr=[6, 2], loc=[3, 2], vel=[3, 2], next_loc=[3, 2], next_vel=[3, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch_geometric.loader.dataloader.DataLoader'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[24, 1], edge_index=[2, 48], edge_attr=[48, 2], loc=[24, 2], vel=[24, 2], next_loc=[24, 2], next_vel=[24, 2], batch=[24], ptr=[9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(loader_train))\n",
    "next(iter(loader_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Location at time 0, velocity at time 0, edge_attr, charges, location at time T\n",
    "# dataset_train[0][0].shape, dataset_train[0][1].shape, dataset_train[0][2].shape, dataset_train[0][3].shape, dataset_train[0][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_train.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_train.get_edges(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------------+-------------------------+----------+\n",
      "| Layer                            | Input Shape                            | Output Shape            | #Param   |\n",
      "|----------------------------------+----------------------------------------+-------------------------+----------|\n",
      "| EGNN_vel                         | [3, 1], [3, 2], [2, 6], [3, 2], [6, 2] | [3, 2], [3, 2]          | 100,677  |\n",
      "| ├─(embedding)Linear              | [3, 1]                                 | [3, 64]                 | 128      |\n",
      "| ├─(gcl_0)E_GCL_vel               | [3, 64], [2, 6], [3, 2], [3, 2]        | [3, 64], [3, 2], [6, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 131]                               | [6, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [6, 131]                               | [6, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 128]                               | [3, 64]                 | 12,416   |\n",
      "| │    │    └─(0)Linear            | [3, 128]                               | [3, 64]                 | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 64]                                | [6, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 64]                                | [3, 1]                  | 4,225    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 1]                  | 65       |\n",
      "| ├─(gcl_1)E_GCL_vel               | [3, 64], [2, 6], [3, 2], [3, 2]        | [3, 64], [3, 2], [6, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 131]                               | [6, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [6, 131]                               | [6, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 128]                               | [3, 64]                 | 12,416   |\n",
      "| │    │    └─(0)Linear            | [3, 128]                               | [3, 64]                 | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 64]                                | [6, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 64]                                | [3, 1]                  | 4,225    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 1]                  | 65       |\n",
      "| ├─(gcl_2)E_GCL_vel               | [3, 64], [2, 6], [3, 2], [3, 2]        | [3, 64], [3, 2], [6, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 131]                               | [6, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [6, 131]                               | [6, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 128]                               | [3, 64]                 | 12,416   |\n",
      "| │    │    └─(0)Linear            | [3, 128]                               | [3, 64]                 | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 64]                                | [6, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 64]                                | [3, 1]                  | 4,225    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 1]                  | 65       |\n",
      "| ├─(vel_predictor)Linear          | [3, 64]                                | [3, 2]                  | 130      |\n",
      "+----------------------------------+----------------------------------------+-------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import summary\n",
    "\n",
    "model = EGNN_vel(in_node_nf=1, in_edge_nf=2, hidden_nf=nf, device=device, n_layers=n_layers, recurrent=True, norm_diff=norm_diff, tanh=tanh).to(device)\n",
    "\n",
    "dummy_data = dataset_train[0]\n",
    "print(summary(model, dummy_data.x, dummy_data.loc, dummy_data.edge_index, dummy_data.vel, dummy_data.edge_attr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_procedure(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf):\n",
    "    results = {'epochs': [], 'losess': []}\n",
    "    best_val_loss = 1e8\n",
    "    best_test_loss = 1e8\n",
    "    best_epoch = 0\n",
    "    for epoch in range(0, epochs):\n",
    "        train(model, optimizer, epoch, loader_train)\n",
    "        if epoch % test_interval == 0:\n",
    "            #train(epoch, loader_train, backprop=False)\n",
    "            val_loss = train(model, optimizer, epoch, loader_val, backprop=False)\n",
    "            test_loss = train(model, optimizer, epoch, loader_test, backprop=False)\n",
    "            results['epochs'].append(epoch)\n",
    "            results['losess'].append(test_loss)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_test_loss = test_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), outf + \"/\" + model_name + f\"/{model_name}.pth\")\n",
    "            print(\"*** Best Val Loss: %.5f \\t Best Test Loss: %.5f \\t Best epoch %d\" % (best_val_loss, best_test_loss, best_epoch))\n",
    "\n",
    "        json_object = json.dumps(results, indent=4)\n",
    "        with open(outf + \"/\" + model_name + \"/losess.json\", \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "    return best_val_loss, best_test_loss, best_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch, loader, backprop=True):\n",
    "    if backprop:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    res = {'epoch': epoch, 'loss': 0, 'coord_reg': 0, 'counter': 0}\n",
    "\n",
    "    # for batch_idx, data in enumerate(loader):\n",
    "    for data in loader:\n",
    "        # print(f\"Data type: {type(data)}\")\n",
    "        # print(f\"Data: {data}\")\n",
    "        # # print(\"\\nBatch idx: \", batch_idx)\n",
    "        # print(\"Data: \", len(data))\n",
    "        # batch_size, n_nodes, _ = data[0].size()\n",
    "        # print(\"Batch size: \", batch_size)\n",
    "        # print(\"Number of nodes: \", n_nodes)\n",
    "        \n",
    "        # data = [d.to(device) for d in data]\n",
    "\n",
    "        # print(\"Data: \", data[0].shape)\n",
    "        # data = [d.view(-1, d.size(2)) for d in data] # Remove the batch dimension\n",
    "        # print(\"Data: \", data[0].shape)\n",
    "\n",
    "        # loc, vel, edge_attr, charges, loc_end = data\n",
    "        loc = data.loc\n",
    "        vel = data.vel\n",
    "        x = data.x\n",
    "        edges = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        loc_end = data.next_loc\n",
    "        vel_end = data.next_vel\n",
    "\n",
    "        # loc, vel, x, edges, edge_attr, loc_end = data # TODO posso farlgli passare anche gli edges\n",
    "\n",
    "        # edges = loader.dataset.get_edges(batch_size, n_nodes) # TODO questa va tolta\n",
    "        edges = [edges[0].to(device), edges[1].to(device)]\n",
    "\n",
    "        \"\"\"\n",
    "            Batch idx:  0\n",
    "            Data: ...\n",
    "            Batch size:  100\n",
    "            Number of nodes:  5\n",
    "            Data:  torch.Size([100, 5, 3])\n",
    "            Data:  torch.Size([500, 3])\n",
    "            train epoch 18 avg loss: 0.01948\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if time_exp:\n",
    "        #     torch.cuda.synchronize()\n",
    "        #     t1 = time.time()\n",
    "        \n",
    "\n",
    "        # nodes = torch.sqrt(torch.sum(vel ** 2, dim=1)).unsqueeze(1).detach()\n",
    "        # rows, cols = edges\n",
    "        # loc_dist = torch.sum((loc[rows] - loc[cols])**2, 1).unsqueeze(1)  # relative distances among locations\n",
    "        # edge_attr = torch.cat([edge_attr, loc_dist], 1).detach()  # concatenate all edge properties\n",
    "\n",
    "        # loc_pred = model(nodes, loc.detach(), edges, vel, edge_attr)\n",
    "        # loc_pred = model(x, loc.detach(), edges, vel, edge_attr)\n",
    "        loc_pred, vel_pred = model(x, loc.detach(), edges, vel, edge_attr)\n",
    "\n",
    "\n",
    "        # if time_exp:\n",
    "        #     torch.cuda.synchronize()\n",
    "        #     t2 = time.time()\n",
    "        #     time_exp_dic['time'] += t2 - t1\n",
    "        #     time_exp_dic['counter'] += 1\n",
    "\n",
    "            # print(\"Forward average time: %.6f\" % (time_exp_dic['time'] / time_exp_dic['counter']))\n",
    "        \n",
    "        # loss = loss_mse(loc_pred, loc_end)\n",
    "        loss = loss_mse(vel_pred, vel_end)\n",
    "\n",
    "        if backprop:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        res['loss'] += loss.item()*batch_size\n",
    "        res['counter'] += batch_size\n",
    "\n",
    "        # if batch_idx % log_interval == 0 and (model == \"se3_transformer\" or model == \"tfn\"):\n",
    "        #     print('===> {} Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(loader.dataset.partition,\n",
    "        #         epoch, batch_idx * batch_size, len(loader.dataset),\n",
    "        #         100. * batch_idx / len(loader),\n",
    "        #         loss.item()))\n",
    "\n",
    "    if not backprop:\n",
    "        prefix = \"==> \"\n",
    "    else:\n",
    "        prefix = \"\"\n",
    "    print('%s epoch %d avg loss: %.5f' % (prefix, epoch, res['loss'] / res['counter']))\n",
    "    # print('%s epoch %d avg loss: %.5f' % (prefix+loader.dataset.partition, epoch, res['loss'] / res['counter']))\n",
    "\n",
    "    return res['loss'] / res['counter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_procedure(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------------+-------------------------+----------+\n",
      "| Layer                            | Input Shape                            | Output Shape            | #Param   |\n",
      "|----------------------------------+----------------------------------------+-------------------------+----------|\n",
      "| EGNN_vel                         | [3, 1], [3, 2], [2, 6], [3, 2], [6, 2] | [3, 2], [3, 2]          | 134,150  |\n",
      "| ├─(embedding)Linear              | [3, 1]                                 | [3, 64]                 | 128      |\n",
      "| ├─(gcl_0)E_GCL_vel               | [3, 64], [2, 6], [3, 2], [3, 2]        | [3, 64], [3, 2], [6, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 131]                               | [6, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [6, 131]                               | [6, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 128]                               | [3, 64]                 | 12,416   |\n",
      "| │    │    └─(0)Linear            | [3, 128]                               | [3, 64]                 | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 64]                                | [6, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 64]                                | [3, 1]                  | 4,225    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 1]                  | 65       |\n",
      "| ├─(gcl_1)E_GCL_vel               | [3, 64], [2, 6], [3, 2], [3, 2]        | [3, 64], [3, 2], [6, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 131]                               | [6, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [6, 131]                               | [6, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 128]                               | [3, 64]                 | 12,416   |\n",
      "| │    │    └─(0)Linear            | [3, 128]                               | [3, 64]                 | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 64]                                | [6, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 64]                                | [3, 1]                  | 4,225    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 1]                  | 65       |\n",
      "| ├─(gcl_2)E_GCL_vel               | [3, 64], [2, 6], [3, 2], [3, 2]        | [3, 64], [3, 2], [6, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 131]                               | [6, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [6, 131]                               | [6, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 128]                               | [3, 64]                 | 12,416   |\n",
      "| │    │    └─(0)Linear            | [3, 128]                               | [3, 64]                 | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 64]                                | [6, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 64]                                | [3, 1]                  | 4,225    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 1]                  | 65       |\n",
      "| ├─(gcl_3)E_GCL_vel               | [3, 64], [2, 6], [3, 2], [3, 2]        | [3, 64], [3, 2], [6, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 131]                               | [6, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [6, 131]                               | [6, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 128]                               | [3, 64]                 | 12,416   |\n",
      "| │    │    └─(0)Linear            | [3, 128]                               | [3, 64]                 | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [6, 64]                                | [6, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 64]                                | [6, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [6, 64]                                | [6, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 64]                                | [6, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 64]                                | [3, 1]                  | 4,225    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [3, 64]                                | [3, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 64]                                | [3, 1]                  | 65       |\n",
      "| ├─(vel_predictor)Linear          | [3, 64]                                | [3, 2]                  | 130      |\n",
      "+----------------------------------+----------------------------------------+-------------------------+----------+\n",
      " epoch 0 avg loss: 0.32837\n",
      "==>  epoch 0 avg loss: 0.30545\n",
      "==>  epoch 0 avg loss: 0.30545\n",
      "*** Best Val Loss: 0.30545 \t Best Test Loss: 0.30545 \t Best epoch 0\n",
      " epoch 1 avg loss: 0.32623\n",
      " epoch 2 avg loss: 0.31418\n",
      " epoch 3 avg loss: 0.31361\n",
      " epoch 4 avg loss: 0.31550\n",
      " epoch 5 avg loss: 0.31643\n",
      "==>  epoch 5 avg loss: 0.30600\n",
      "==>  epoch 5 avg loss: 0.30600\n",
      "*** Best Val Loss: 0.30545 \t Best Test Loss: 0.30545 \t Best epoch 0\n",
      " epoch 6 avg loss: 0.31351\n",
      " epoch 7 avg loss: 0.32668\n",
      " epoch 8 avg loss: 0.31389\n",
      " epoch 9 avg loss: 0.31413\n",
      " epoch 10 avg loss: 0.31471\n",
      "==>  epoch 10 avg loss: 0.29881\n",
      "==>  epoch 10 avg loss: 0.29881\n",
      "*** Best Val Loss: 0.29881 \t Best Test Loss: 0.29881 \t Best epoch 10\n",
      " epoch 11 avg loss: 0.31446\n",
      " epoch 12 avg loss: 0.31441\n",
      " epoch 13 avg loss: 0.31449\n",
      " epoch 14 avg loss: 0.31410\n",
      " epoch 15 avg loss: 0.31483\n",
      "==>  epoch 15 avg loss: 0.30187\n",
      "==>  epoch 15 avg loss: 0.30187\n",
      "*** Best Val Loss: 0.29881 \t Best Test Loss: 0.29881 \t Best epoch 10\n",
      " epoch 16 avg loss: 0.31399\n",
      " epoch 17 avg loss: 0.31427\n",
      " epoch 18 avg loss: 0.31433\n",
      " epoch 19 avg loss: 0.32522\n",
      " epoch 20 avg loss: 0.31306\n",
      "==>  epoch 20 avg loss: 0.29794\n",
      "==>  epoch 20 avg loss: 0.29794\n",
      "*** Best Val Loss: 0.29794 \t Best Test Loss: 0.29794 \t Best epoch 20\n",
      " epoch 21 avg loss: 0.31452\n",
      " epoch 22 avg loss: 0.31913\n",
      " epoch 23 avg loss: 0.31306\n",
      " epoch 24 avg loss: 0.31346\n",
      " epoch 25 avg loss: 0.31339\n",
      "==>  epoch 25 avg loss: 0.29846\n",
      "==>  epoch 25 avg loss: 0.29846\n",
      "*** Best Val Loss: 0.29794 \t Best Test Loss: 0.29794 \t Best epoch 20\n",
      " epoch 26 avg loss: 0.31363\n",
      " epoch 27 avg loss: 0.31346\n",
      " epoch 28 avg loss: 0.53365\n",
      " epoch 29 avg loss: 0.31326\n",
      " epoch 30 avg loss: 0.31286\n",
      "==>  epoch 30 avg loss: 0.29758\n",
      "==>  epoch 30 avg loss: 0.29758\n",
      "*** Best Val Loss: 0.29758 \t Best Test Loss: 0.29758 \t Best epoch 30\n",
      " epoch 31 avg loss: 0.31282\n",
      " epoch 32 avg loss: 0.31267\n",
      " epoch 33 avg loss: 1163.06916\n",
      " epoch 34 avg loss: 0.31250\n",
      " epoch 35 avg loss: 0.33666\n",
      "==>  epoch 35 avg loss: 0.29679\n",
      "==>  epoch 35 avg loss: 0.29679\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 36 avg loss: 0.31142\n",
      " epoch 37 avg loss: 0.31140\n",
      " epoch 38 avg loss: 0.31107\n",
      " epoch 39 avg loss: 0.31164\n",
      " epoch 40 avg loss: 0.31169\n",
      "==>  epoch 40 avg loss: 0.29836\n",
      "==>  epoch 40 avg loss: 0.29836\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 41 avg loss: 0.35009\n",
      " epoch 42 avg loss: 0.31188\n",
      " epoch 43 avg loss: 0.31592\n",
      " epoch 44 avg loss: 0.35469\n",
      " epoch 45 avg loss: 0.31276\n",
      "==>  epoch 45 avg loss: 0.30084\n",
      "==>  epoch 45 avg loss: 0.30084\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 46 avg loss: 0.31279\n",
      " epoch 47 avg loss: 0.31330\n",
      " epoch 48 avg loss: 0.31325\n",
      " epoch 49 avg loss: 0.31336\n",
      " epoch 50 avg loss: 0.31316\n",
      "==>  epoch 50 avg loss: 0.29916\n",
      "==>  epoch 50 avg loss: 0.29916\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 51 avg loss: 2.48706\n",
      " epoch 52 avg loss: 0.31328\n",
      " epoch 53 avg loss: 0.31313\n",
      " epoch 54 avg loss: 0.31333\n",
      " epoch 55 avg loss: 0.31322\n",
      "==>  epoch 55 avg loss: 0.29891\n",
      "==>  epoch 55 avg loss: 0.29891\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 56 avg loss: 0.31301\n",
      " epoch 57 avg loss: 0.31293\n",
      " epoch 58 avg loss: 0.31265\n",
      " epoch 59 avg loss: 43.04970\n",
      " epoch 60 avg loss: 0.31349\n",
      "==>  epoch 60 avg loss: 0.29869\n",
      "==>  epoch 60 avg loss: 0.29869\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 61 avg loss: 0.31361\n",
      " epoch 62 avg loss: 0.31365\n",
      " epoch 63 avg loss: 0.31368\n",
      " epoch 64 avg loss: 0.31372\n",
      " epoch 65 avg loss: 0.31359\n",
      "==>  epoch 65 avg loss: 0.29929\n",
      "==>  epoch 65 avg loss: 0.29929\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 66 avg loss: 0.31320\n",
      " epoch 67 avg loss: 0.31321\n",
      " epoch 68 avg loss: 3.29850\n",
      " epoch 69 avg loss: 0.31229\n",
      " epoch 70 avg loss: 0.31229\n",
      "==>  epoch 70 avg loss: 0.29909\n",
      "==>  epoch 70 avg loss: 0.29909\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 71 avg loss: 0.31227\n",
      " epoch 72 avg loss: 0.31280\n",
      " epoch 73 avg loss: 0.31282\n",
      " epoch 74 avg loss: 0.31317\n",
      " epoch 75 avg loss: 0.31275\n",
      "==>  epoch 75 avg loss: 0.29829\n",
      "==>  epoch 75 avg loss: 0.29829\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 76 avg loss: 0.35645\n",
      " epoch 77 avg loss: 0.31311\n",
      " epoch 78 avg loss: 15.53610\n",
      " epoch 79 avg loss: 0.31380\n",
      " epoch 80 avg loss: 0.31302\n",
      "==>  epoch 80 avg loss: 0.29874\n",
      "==>  epoch 80 avg loss: 0.29874\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 81 avg loss: 0.31256\n",
      " epoch 82 avg loss: 0.31215\n",
      " epoch 83 avg loss: 0.31205\n",
      " epoch 84 avg loss: 0.31235\n",
      " epoch 85 avg loss: 0.31268\n",
      "==>  epoch 85 avg loss: 0.29977\n",
      "==>  epoch 85 avg loss: 0.29977\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 86 avg loss: 0.31248\n",
      " epoch 87 avg loss: 0.31760\n",
      " epoch 88 avg loss: 0.31279\n",
      " epoch 89 avg loss: 0.31255\n",
      " epoch 90 avg loss: 0.31310\n",
      "==>  epoch 90 avg loss: 0.29940\n",
      "==>  epoch 90 avg loss: 0.29940\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 91 avg loss: 0.31198\n",
      " epoch 92 avg loss: 0.31238\n",
      " epoch 93 avg loss: 0.31228\n",
      " epoch 94 avg loss: 0.31186\n",
      " epoch 95 avg loss: 0.35758\n",
      "==>  epoch 95 avg loss: 0.29833\n",
      "==>  epoch 95 avg loss: 0.29833\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 96 avg loss: 0.31368\n",
      " epoch 97 avg loss: 0.31175\n",
      " epoch 98 avg loss: 0.31169\n",
      " epoch 99 avg loss: 0.31140\n",
      " epoch 100 avg loss: 0.31171\n",
      "==>  epoch 100 avg loss: 0.30107\n",
      "==>  epoch 100 avg loss: 0.30107\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 101 avg loss: 0.31111\n",
      " epoch 102 avg loss: 0.31173\n",
      " epoch 103 avg loss: 717.37101\n",
      " epoch 104 avg loss: 0.31651\n",
      " epoch 105 avg loss: 0.31337\n",
      "==>  epoch 105 avg loss: 0.29859\n",
      "==>  epoch 105 avg loss: 0.29859\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 106 avg loss: 0.31247\n",
      " epoch 107 avg loss: 0.31270\n",
      " epoch 108 avg loss: 0.31246\n",
      " epoch 109 avg loss: 0.31176\n",
      " epoch 110 avg loss: 0.31134\n",
      "==>  epoch 110 avg loss: 0.29818\n",
      "==>  epoch 110 avg loss: 0.29818\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 111 avg loss: 0.31211\n",
      " epoch 112 avg loss: 0.32061\n",
      " epoch 113 avg loss: 0.31165\n",
      " epoch 114 avg loss: 0.31152\n",
      " epoch 115 avg loss: 0.31172\n",
      "==>  epoch 115 avg loss: 0.29966\n",
      "==>  epoch 115 avg loss: 0.29966\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 116 avg loss: 0.31212\n",
      " epoch 117 avg loss: 0.31218\n",
      " epoch 118 avg loss: 0.31263\n",
      " epoch 119 avg loss: 0.31232\n",
      " epoch 120 avg loss: 0.31262\n",
      "==>  epoch 120 avg loss: 0.29825\n",
      "==>  epoch 120 avg loss: 0.29825\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 121 avg loss: 0.31250\n",
      " epoch 122 avg loss: 0.31177\n",
      " epoch 123 avg loss: 0.31187\n",
      " epoch 124 avg loss: 0.31250\n",
      " epoch 125 avg loss: 0.31211\n",
      "==>  epoch 125 avg loss: 0.30280\n",
      "==>  epoch 125 avg loss: 0.30280\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 126 avg loss: 0.31264\n",
      " epoch 127 avg loss: 0.31489\n",
      " epoch 128 avg loss: 0.31182\n",
      " epoch 129 avg loss: 0.31184\n",
      " epoch 130 avg loss: 0.31218\n",
      "==>  epoch 130 avg loss: 0.29791\n",
      "==>  epoch 130 avg loss: 0.29791\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 131 avg loss: 0.31212\n",
      " epoch 132 avg loss: 0.31346\n",
      " epoch 133 avg loss: 0.31263\n",
      " epoch 134 avg loss: 0.31193\n",
      " epoch 135 avg loss: 0.31246\n",
      "==>  epoch 135 avg loss: 0.29849\n",
      "==>  epoch 135 avg loss: 0.29849\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 136 avg loss: 0.31351\n",
      " epoch 137 avg loss: 0.31314\n",
      " epoch 138 avg loss: 0.31356\n",
      " epoch 139 avg loss: 0.31339\n",
      " epoch 140 avg loss: 0.31348\n",
      "==>  epoch 140 avg loss: 0.29853\n",
      "==>  epoch 140 avg loss: 0.29853\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 141 avg loss: 0.31318\n",
      " epoch 142 avg loss: 0.31373\n",
      " epoch 143 avg loss: 0.31304\n",
      " epoch 144 avg loss: 0.31319\n",
      " epoch 145 avg loss: 0.31301\n",
      "==>  epoch 145 avg loss: 0.29881\n",
      "==>  epoch 145 avg loss: 0.29881\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 146 avg loss: 0.31250\n",
      " epoch 147 avg loss: 0.31304\n",
      " epoch 148 avg loss: 0.31306\n",
      " epoch 149 avg loss: 0.31213\n",
      " epoch 150 avg loss: 0.31130\n",
      "==>  epoch 150 avg loss: 0.29992\n",
      "==>  epoch 150 avg loss: 0.29992\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 151 avg loss: 0.31175\n",
      " epoch 152 avg loss: 0.31154\n",
      " epoch 153 avg loss: 0.31249\n",
      " epoch 154 avg loss: 0.31132\n",
      " epoch 155 avg loss: 0.31116\n",
      "==>  epoch 155 avg loss: 0.29943\n",
      "==>  epoch 155 avg loss: 0.29943\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 156 avg loss: 0.31259\n",
      " epoch 157 avg loss: 0.31180\n",
      " epoch 158 avg loss: 0.31142\n",
      " epoch 159 avg loss: 0.31379\n",
      " epoch 160 avg loss: 0.31301\n",
      "==>  epoch 160 avg loss: 0.30007\n",
      "==>  epoch 160 avg loss: 0.30007\n",
      "*** Best Val Loss: 0.29679 \t Best Test Loss: 0.29679 \t Best epoch 35\n",
      " epoch 161 avg loss: 0.31251\n",
      " epoch 162 avg loss: 0.31311\n",
      " epoch 163 avg loss: 0.31197\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary(model, dummy_data\u001b[38;5;241m.\u001b[39mx, dummy_data\u001b[38;5;241m.\u001b[39mloc, dummy_data\u001b[38;5;241m.\u001b[39medge_index, dummy_data\u001b[38;5;241m.\u001b[39mvel, dummy_data\u001b[38;5;241m.\u001b[39medge_attr))\n\u001b[1;32m     46\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtraining_procedure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36mtraining_procedure\u001b[0;34m(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf)\u001b[0m\n\u001b[1;32m      5\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs):\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m test_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m#train(epoch, loader_train, backprop=False)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m train(model, optimizer, epoch, loader_val, backprop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[27], line 78\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epoch, loader, backprop)\u001b[0m\n\u001b[1;32m     75\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_mse(vel_pred, vel_end)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backprop:\n\u001b[0;32m---> 78\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     81\u001b[0m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mbatch_size\n",
      "File \u001b[0;32m~/Desktop/egnn-main/.venv/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/egnn-main/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/egnn-main/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Smaller batch size\n",
    "model_name = 'egnn_task2_2'\n",
    "batch_size = 4\n",
    "epochs = 300\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "log_interval = 1\n",
    "test_interval = 5\n",
    "outf = './logs'\n",
    "lr = 5e-4\n",
    "nf = 64\n",
    "attention = 0\n",
    "n_layers = 4\n",
    "degree = 2\n",
    "max_training_samples = 3000\n",
    "dataset = \"nbody_small\"\n",
    "sweep_training = 0\n",
    "time_exp = 0\n",
    "weight_decay = 1e-12\n",
    "div = 1\n",
    "norm_diff = False\n",
    "tanh = False\n",
    "\n",
    "time_exp_dic = {'time': 0, 'counter': 0}\n",
    "\n",
    "# Create log directory\n",
    "try:\n",
    "    os.makedirs(outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(outf + \"/\" + model_name)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# __________________________________________________________________________________________________ #\n",
    "\n",
    "model = EGNN_vel(in_node_nf=1, in_edge_nf=2, hidden_nf=nf, device=device, n_layers=n_layers, recurrent=True, norm_diff=norm_diff, tanh=tanh).to(device)\n",
    "\n",
    "dummy_data = dataset_train[0]\n",
    "print(summary(model, dummy_data.x, dummy_data.loc, dummy_data.edge_index, dummy_data.vel, dummy_data.edge_attr))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "training_procedure(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------------+-------------------------+----------+\n",
      "| Layer                            | Input Shape                            | Output Shape            | #Param   |\n",
      "|----------------------------------+----------------------------------------+-------------------------+----------|\n",
      "| EGNN_vel                         | [3, 1], [3, 2], [2, 6], [3, 2], [6, 2] | [3, 2], [3, 2]          | 34,310   |\n",
      "| ├─(embedding)Linear              | [3, 1]                                 | [3, 32]                 | 64       |\n",
      "| ├─(gcl_0)E_GCL_vel               | [3, 32], [2, 6], [3, 2], [3, 2]        | [3, 32], [3, 2], [6, 2] | 8,545    |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 67]                                | [6, 32]                 | 3,232    |\n",
      "| │    │    └─(0)Linear            | [6, 67]                                | [6, 32]                 | 2,176    |\n",
      "| │    │    └─(1)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 32]                                | [6, 32]                 | 1,056    |\n",
      "| │    │    └─(3)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 64]                                | [3, 32]                 | 3,136    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 32]                 | 2,080    |\n",
      "| │    │    └─(1)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 32]                                | [3, 32]                 | 1,056    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 32]                                | [6, 1]                  | 1,088    |\n",
      "| │    │    └─(0)Linear            | [6, 32]                                | [6, 32]                 | 1,056    |\n",
      "| │    │    └─(1)SiLU              | [3, 32]                                | [3, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 32]                                | [6, 1]                  | 32       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 32]                                | [3, 1]                  | 1,089    |\n",
      "| │    │    └─(0)Linear            | [3, 32]                                | [3, 32]                 | 1,056    |\n",
      "| │    │    └─(1)SiLU              | [3, 32]                                | [3, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 32]                                | [3, 1]                  | 33       |\n",
      "| ├─(gcl_1)E_GCL_vel               | [3, 32], [2, 6], [3, 2], [3, 2]        | [3, 32], [3, 2], [6, 2] | 8,545    |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 67]                                | [6, 32]                 | 3,232    |\n",
      "| │    │    └─(0)Linear            | [6, 67]                                | [6, 32]                 | 2,176    |\n",
      "| │    │    └─(1)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 32]                                | [6, 32]                 | 1,056    |\n",
      "| │    │    └─(3)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 64]                                | [3, 32]                 | 3,136    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 32]                 | 2,080    |\n",
      "| │    │    └─(1)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 32]                                | [3, 32]                 | 1,056    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 32]                                | [6, 1]                  | 1,088    |\n",
      "| │    │    └─(0)Linear            | [6, 32]                                | [6, 32]                 | 1,056    |\n",
      "| │    │    └─(1)SiLU              | [3, 32]                                | [3, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 32]                                | [6, 1]                  | 32       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 32]                                | [3, 1]                  | 1,089    |\n",
      "| │    │    └─(0)Linear            | [3, 32]                                | [3, 32]                 | 1,056    |\n",
      "| │    │    └─(1)SiLU              | [3, 32]                                | [3, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 32]                                | [3, 1]                  | 33       |\n",
      "| ├─(gcl_2)E_GCL_vel               | [3, 32], [2, 6], [3, 2], [3, 2]        | [3, 32], [3, 2], [6, 2] | 8,545    |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 67]                                | [6, 32]                 | 3,232    |\n",
      "| │    │    └─(0)Linear            | [6, 67]                                | [6, 32]                 | 2,176    |\n",
      "| │    │    └─(1)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 32]                                | [6, 32]                 | 1,056    |\n",
      "| │    │    └─(3)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 64]                                | [3, 32]                 | 3,136    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 32]                 | 2,080    |\n",
      "| │    │    └─(1)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 32]                                | [3, 32]                 | 1,056    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 32]                                | [6, 1]                  | 1,088    |\n",
      "| │    │    └─(0)Linear            | [6, 32]                                | [6, 32]                 | 1,056    |\n",
      "| │    │    └─(1)SiLU              | [3, 32]                                | [3, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 32]                                | [6, 1]                  | 32       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 32]                                | [3, 1]                  | 1,089    |\n",
      "| │    │    └─(0)Linear            | [3, 32]                                | [3, 32]                 | 1,056    |\n",
      "| │    │    └─(1)SiLU              | [3, 32]                                | [3, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 32]                                | [3, 1]                  | 33       |\n",
      "| ├─(gcl_3)E_GCL_vel               | [3, 32], [2, 6], [3, 2], [3, 2]        | [3, 32], [3, 2], [6, 2] | 8,545    |\n",
      "| │    └─(edge_mlp)Sequential      | [6, 67]                                | [6, 32]                 | 3,232    |\n",
      "| │    │    └─(0)Linear            | [6, 67]                                | [6, 32]                 | 2,176    |\n",
      "| │    │    └─(1)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 32]                                | [6, 32]                 | 1,056    |\n",
      "| │    │    └─(3)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [3, 64]                                | [3, 32]                 | 3,136    |\n",
      "| │    │    └─(0)Linear            | [3, 64]                                | [3, 32]                 | 2,080    |\n",
      "| │    │    └─(1)SiLU              | [6, 32]                                | [6, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 32]                                | [3, 32]                 | 1,056    |\n",
      "| │    └─(coord_mlp)Sequential     | [6, 32]                                | [6, 1]                  | 1,088    |\n",
      "| │    │    └─(0)Linear            | [6, 32]                                | [6, 32]                 | 1,056    |\n",
      "| │    │    └─(1)SiLU              | [3, 32]                                | [3, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [6, 32]                                | [6, 1]                  | 32       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [3, 32]                                | [3, 1]                  | 1,089    |\n",
      "| │    │    └─(0)Linear            | [3, 32]                                | [3, 32]                 | 1,056    |\n",
      "| │    │    └─(1)SiLU              | [3, 32]                                | [3, 32]                 | --       |\n",
      "| │    │    └─(2)Linear            | [3, 32]                                | [3, 1]                  | 33       |\n",
      "| ├─(vel_predictor)Linear          | [3, 32]                                | [3, 2]                  | 66       |\n",
      "+----------------------------------+----------------------------------------+-------------------------+----------+\n",
      " epoch 0 avg loss: 0.89818\n",
      "==>  epoch 0 avg loss: 0.30301\n",
      "==>  epoch 0 avg loss: 0.30301\n",
      "*** Best Val Loss: 0.30301 \t Best Test Loss: 0.30301 \t Best epoch 0\n",
      " epoch 1 avg loss: 0.31789\n",
      " epoch 2 avg loss: 0.31753\n",
      " epoch 3 avg loss: 0.31634\n",
      " epoch 4 avg loss: 0.31460\n",
      " epoch 5 avg loss: 0.31370\n",
      "==>  epoch 5 avg loss: 0.29819\n",
      "==>  epoch 5 avg loss: 0.29819\n",
      "*** Best Val Loss: 0.29819 \t Best Test Loss: 0.29819 \t Best epoch 5\n",
      " epoch 6 avg loss: 0.55133\n",
      " epoch 7 avg loss: 0.31223\n",
      " epoch 8 avg loss: 0.31196\n",
      " epoch 9 avg loss: 0.31896\n",
      " epoch 10 avg loss: 0.31203\n",
      "==>  epoch 10 avg loss: 0.29673\n",
      "==>  epoch 10 avg loss: 0.29673\n",
      "*** Best Val Loss: 0.29673 \t Best Test Loss: 0.29673 \t Best epoch 10\n",
      " epoch 11 avg loss: 0.31177\n",
      " epoch 12 avg loss: 0.31159\n",
      " epoch 13 avg loss: 0.31103\n",
      " epoch 14 avg loss: 0.31135\n",
      " epoch 15 avg loss: 0.31078\n",
      "==>  epoch 15 avg loss: 0.29627\n",
      "==>  epoch 15 avg loss: 0.29627\n",
      "*** Best Val Loss: 0.29627 \t Best Test Loss: 0.29627 \t Best epoch 15\n",
      " epoch 16 avg loss: 0.31101\n",
      " epoch 17 avg loss: 0.31183\n",
      " epoch 18 avg loss: 0.31076\n",
      " epoch 19 avg loss: 0.31152\n",
      " epoch 20 avg loss: 0.30954\n",
      "==>  epoch 20 avg loss: 0.29570\n",
      "==>  epoch 20 avg loss: 0.29570\n",
      "*** Best Val Loss: 0.29570 \t Best Test Loss: 0.29570 \t Best epoch 20\n",
      " epoch 21 avg loss: 0.31302\n"
     ]
    }
   ],
   "source": [
    "# Smaller model\n",
    "model_name = 'egnn_task2_3'\n",
    "batch_size = 16\n",
    "epochs = 300\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "log_interval = 1\n",
    "test_interval = 5\n",
    "outf = './logs'\n",
    "lr = 5e-4\n",
    "nf = 32\n",
    "attention = 0\n",
    "n_layers = 4\n",
    "degree = 2\n",
    "max_training_samples = 3000\n",
    "dataset = \"nbody_small\"\n",
    "sweep_training = 0\n",
    "time_exp = 0\n",
    "weight_decay = 1e-12\n",
    "div = 1\n",
    "norm_diff = False\n",
    "tanh = False\n",
    "\n",
    "time_exp_dic = {'time': 0, 'counter': 0}\n",
    "\n",
    "# Create log directory\n",
    "try:\n",
    "    os.makedirs(outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(outf + \"/\" + model_name)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# __________________________________________________________________________________________________ #\n",
    "\n",
    "model = EGNN_vel(in_node_nf=1, in_edge_nf=2, hidden_nf=nf, device=device, n_layers=n_layers, recurrent=True, norm_diff=norm_diff, tanh=tanh).to(device)\n",
    "\n",
    "dummy_data = dataset_train[0]\n",
    "print(summary(model, dummy_data.x, dummy_data.loc, dummy_data.edge_index, dummy_data.vel, dummy_data.edge_attr))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "training_procedure(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowe lr\n",
    "model_name = 'egnn_task2_4'\n",
    "batch_size = 16\n",
    "epochs = 300\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "log_interval = 1\n",
    "test_interval = 5\n",
    "outf = './logs'\n",
    "lr = 1e-4\n",
    "nf = 64\n",
    "attention = 0\n",
    "n_layers = 4\n",
    "degree = 2\n",
    "max_training_samples = 3000\n",
    "dataset = \"nbody_small\"\n",
    "sweep_training = 0\n",
    "time_exp = 0\n",
    "weight_decay = 1e-12\n",
    "div = 1\n",
    "norm_diff = False\n",
    "tanh = False\n",
    "\n",
    "time_exp_dic = {'time': 0, 'counter': 0}\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Create log directory\n",
    "try:\n",
    "    os.makedirs(outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(outf + \"/\" + model_name)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# __________________________________________________________________________________________________ #\n",
    "\n",
    "model = EGNN_vel(in_node_nf=1, in_edge_nf=2, hidden_nf=nf, device=device, n_layers=n_layers, recurrent=True, norm_diff=norm_diff, tanh=tanh).to(device)\n",
    "\n",
    "dummy_data = dataset_train[0]\n",
    "print(summary(model, dummy_data.x, dummy_data.loc, dummy_data.edge_index, dummy_data.vel, dummy_data.edge_attr))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "training_procedure(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
