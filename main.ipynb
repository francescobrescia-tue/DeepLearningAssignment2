{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    \"\"\"Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\"\"\"\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from n_body_system.dataset_nbody import NBodyDataset\n",
    "import dataset as dataset_nbody\n",
    "from dataset import NBodyDataset\n",
    "# from n_body_system.model import EGNN_vel\n",
    "from EGNN import EGNN_vel\n",
    "# from functions_utils import unsorted_segment_mean\n",
    "import os\n",
    "from torch import nn, optim\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'egnn_task2'\n",
    "batch_size = 100\n",
    "epochs = 1000\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "log_interval = 1\n",
    "test_interval = 5\n",
    "outf = './logs'\n",
    "lr = 5e-4\n",
    "nf = 64\n",
    "attention = 0\n",
    "n_layers = 4\n",
    "degree = 2\n",
    "max_training_samples = 3000\n",
    "dataset = \"nbody_small\"\n",
    "sweep_training = 0\n",
    "time_exp = 0\n",
    "weight_decay = 1e-12\n",
    "div = 1\n",
    "norm_diff = False\n",
    "tanh = False\n",
    "\n",
    "time_exp_dic = {'time': 0, 'counter': 0}\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log directory\n",
    "try:\n",
    "    os.makedirs(outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(outf + \"/\" + model_name)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "loss_mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity_attr(loc, vel, rows, cols):\n",
    "\n",
    "    diff = loc[cols] - loc[rows]\n",
    "    norm = torch.norm(diff, p=2, dim=1).unsqueeze(1)\n",
    "    u = diff/norm\n",
    "    va, vb = vel[rows] * u, vel[cols] * u\n",
    "    va, vb = torch.sum(va, dim=1).unsqueeze(1), torch.sum(vb, dim=1).unsqueeze(1)\n",
    "    return va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Datasets and loaders\n",
    "# dataset_train = NBodyDataset(partition='train', dataset_name=dataset,\n",
    "#                                  max_samples=max_training_samples)\n",
    "# loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# dataset_val = NBodyDataset(partition='val', dataset_name=\"nbody_small\")\n",
    "# loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# dataset_test = NBodyDataset(partition='test', dataset_name=\"nbody_small\")\n",
    "# loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "paths = glob.glob(\"data/task1_2/train/*.npz\")\n",
    "\n",
    "dataset_train = NBodyDataset(paths)\n",
    "dataset_size = len(dataset_train)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.2 * dataset_size)\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(dataset_train, [train_size, val_size])\n",
    "\n",
    "loader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "loader_val = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "paths_test = glob.glob(\"data/task1_2/test/*.npz\")\n",
    "test_data = NBodyDataset(paths_test)\n",
    "loader_test = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[4, 1], edge_index=[2, 12], edge_attr=[12, 2], loc=[4, 2], vel=[4, 2], next_loc=[4, 2], next_vel=[4, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch_geometric.loader.dataloader.DataLoader'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[339, 1], edge_index=[2, 834], edge_attr=[834, 2], loc=[339, 2], vel=[339, 2], next_loc=[339, 2], next_vel=[339, 2], batch=[339], ptr=[101])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(loader_train))\n",
    "next(iter(loader_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Location at time 0, velocity at time 0, edge_attr, charges, location at time T\n",
    "# dataset_train[0][0].shape, dataset_train[0][1].shape, dataset_train[0][2].shape, dataset_train[0][3].shape, dataset_train[0][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_train.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_train.get_edges(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------------------------------------+--------------------------+----------+\n",
      "| Layer                            | Input Shape                              | Output Shape             | #Param   |\n",
      "|----------------------------------+------------------------------------------+--------------------------+----------|\n",
      "| EGNN_vel                         | [4, 1], [4, 2], [2, 12], [4, 2], [12, 2] | [4, 2], [4, 2]           | 134,150  |\n",
      "| ├─(embedding)Linear              | [4, 1]                                   | [4, 64]                  | 128      |\n",
      "| ├─(gcl_0)E_GCL_vel               | [4, 64], [2, 12], [4, 2], [4, 2]         | [4, 64], [4, 2], [12, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [12, 131]                                | [12, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [12, 131]                                | [12, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [12, 64]                                 | [12, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [4, 128]                                 | [4, 64]                  | 12,416   |\n",
      "| │    │    └─(0)Linear            | [4, 128]                                 | [4, 64]                  | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [4, 64]                                  | [4, 64]                  | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [12, 64]                                 | [12, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [12, 64]                                 | [12, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [4, 64]                                  | [4, 64]                  | --       |\n",
      "| │    │    └─(2)Linear            | [12, 64]                                 | [12, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [4, 64]                                  | [4, 1]                   | 4,225    |\n",
      "| │    │    └─(0)Linear            | [4, 64]                                  | [4, 64]                  | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [4, 64]                                  | [4, 64]                  | --       |\n",
      "| │    │    └─(2)Linear            | [4, 64]                                  | [4, 1]                   | 65       |\n",
      "| ├─(gcl_1)E_GCL_vel               | [4, 64], [2, 12], [4, 2], [4, 2]         | [4, 64], [4, 2], [12, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [12, 131]                                | [12, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [12, 131]                                | [12, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [12, 64]                                 | [12, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [4, 128]                                 | [4, 64]                  | 12,416   |\n",
      "| │    │    └─(0)Linear            | [4, 128]                                 | [4, 64]                  | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [4, 64]                                  | [4, 64]                  | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [12, 64]                                 | [12, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [12, 64]                                 | [12, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [4, 64]                                  | [4, 64]                  | --       |\n",
      "| │    │    └─(2)Linear            | [12, 64]                                 | [12, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [4, 64]                                  | [4, 1]                   | 4,225    |\n",
      "| │    │    └─(0)Linear            | [4, 64]                                  | [4, 64]                  | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [4, 64]                                  | [4, 64]                  | --       |\n",
      "| │    │    └─(2)Linear            | [4, 64]                                  | [4, 1]                   | 65       |\n",
      "| ├─(gcl_2)E_GCL_vel               | [4, 64], [2, 12], [4, 2], [4, 2]         | [4, 64], [4, 2], [12, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [12, 131]                                | [12, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [12, 131]                                | [12, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [12, 64]                                 | [12, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [4, 128]                                 | [4, 64]                  | 12,416   |\n",
      "| │    │    └─(0)Linear            | [4, 128]                                 | [4, 64]                  | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [4, 64]                                  | [4, 64]                  | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [12, 64]                                 | [12, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [12, 64]                                 | [12, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [4, 64]                                  | [4, 64]                  | --       |\n",
      "| │    │    └─(2)Linear            | [12, 64]                                 | [12, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [4, 64]                                  | [4, 1]                   | 4,225    |\n",
      "| │    │    └─(0)Linear            | [4, 64]                                  | [4, 64]                  | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [4, 64]                                  | [4, 64]                  | --       |\n",
      "| │    │    └─(2)Linear            | [4, 64]                                  | [4, 1]                   | 65       |\n",
      "| ├─(gcl_3)E_GCL_vel               | [4, 64], [2, 12], [4, 2], [4, 2]         | [4, 64], [4, 2], [12, 2] | 33,473   |\n",
      "| │    └─(edge_mlp)Sequential      | [12, 131]                                | [12, 64]                 | 12,608   |\n",
      "| │    │    └─(0)Linear            | [12, 131]                                | [12, 64]                 | 8,448    |\n",
      "| │    │    └─(1)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [12, 64]                                 | [12, 64]                 | 4,160    |\n",
      "| │    │    └─(3)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    └─(node_mlp)Sequential      | [4, 128]                                 | [4, 64]                  | 12,416   |\n",
      "| │    │    └─(0)Linear            | [4, 128]                                 | [4, 64]                  | 8,256    |\n",
      "| │    │    └─(1)SiLU              | [12, 64]                                 | [12, 64]                 | --       |\n",
      "| │    │    └─(2)Linear            | [4, 64]                                  | [4, 64]                  | 4,160    |\n",
      "| │    └─(coord_mlp)Sequential     | [12, 64]                                 | [12, 1]                  | 4,224    |\n",
      "| │    │    └─(0)Linear            | [12, 64]                                 | [12, 64]                 | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [4, 64]                                  | [4, 64]                  | --       |\n",
      "| │    │    └─(2)Linear            | [12, 64]                                 | [12, 1]                  | 64       |\n",
      "| │    └─(coord_mlp_vel)Sequential | [4, 64]                                  | [4, 1]                   | 4,225    |\n",
      "| │    │    └─(0)Linear            | [4, 64]                                  | [4, 64]                  | 4,160    |\n",
      "| │    │    └─(1)SiLU              | [4, 64]                                  | [4, 64]                  | --       |\n",
      "| │    │    └─(2)Linear            | [4, 64]                                  | [4, 1]                   | 65       |\n",
      "| ├─(vel_predictor)Linear          | [4, 64]                                  | [4, 2]                   | 130      |\n",
      "+----------------------------------+------------------------------------------+--------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import summary\n",
    "\n",
    "model = EGNN_vel(in_node_nf=1, in_edge_nf=2, hidden_nf=nf, device=device, n_layers=n_layers, recurrent=True, norm_diff=norm_diff, tanh=tanh).to(device)\n",
    "\n",
    "dummy_data = dataset[0]\n",
    "x = dummy_data.x.to(device)\n",
    "loc = dummy_data.loc.to(device)\n",
    "edge_index = dummy_data.edge_index.to(device)\n",
    "vel = dummy_data.vel.to(device)\n",
    "edge_attr = dummy_data.edge_attr.to(device)\n",
    "print(summary(model, x, loc, edge_index, vel, edge_attr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_procedure(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf):\n",
    "    results = {'epochs': [], 'losess': []}\n",
    "    best_val_loss = 1e8\n",
    "    best_test_loss = 1e8\n",
    "    best_epoch = 0\n",
    "    for epoch in range(0, epochs):\n",
    "        train(model, optimizer, epoch, loader_train)\n",
    "        if epoch % test_interval == 0:\n",
    "            #train(epoch, loader_train, backprop=False)\n",
    "            val_loss = train(model, optimizer, epoch, loader_val, backprop=False)\n",
    "            test_loss = train(model, optimizer, epoch, loader_test, backprop=False)\n",
    "            results['epochs'].append(epoch)\n",
    "            results['losess'].append(test_loss)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_test_loss = test_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), outf + \"/\" + model_name + f\"/{model_name}.pth\")\n",
    "            print(\"*** Best Val Loss: %.5f \\t Best Test Loss: %.5f \\t Best epoch %d\" % (best_val_loss, best_test_loss, best_epoch))\n",
    "\n",
    "        json_object = json.dumps(results, indent=4)\n",
    "        with open(outf + \"/\" + model_name + \"/losess.json\", \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "    return best_val_loss, best_test_loss, best_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch, loader, backprop=True):\n",
    "    if backprop:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    res = {'epoch': epoch, 'loss': 0, 'coord_reg': 0, 'counter': 0}\n",
    "\n",
    "    # for batch_idx, data in enumerate(loader):\n",
    "    for data in loader:\n",
    "        # print(f\"Data type: {type(data)}\")\n",
    "        # print(f\"Data: {data}\")\n",
    "        # # print(\"\\nBatch idx: \", batch_idx)\n",
    "        # print(\"Data: \", len(data))\n",
    "        # batch_size, n_nodes, _ = data[0].size()\n",
    "        # print(\"Batch size: \", batch_size)\n",
    "        # print(\"Number of nodes: \", n_nodes)\n",
    "        \n",
    "        # data = [d.to(device) for d in data]\n",
    "\n",
    "        # print(\"Data: \", data[0].shape)\n",
    "        # data = [d.view(-1, d.size(2)) for d in data] # Remove the batch dimension\n",
    "        # print(\"Data: \", data[0].shape)\n",
    "\n",
    "        # loc, vel, edge_attr, charges, loc_end = data\n",
    "        loc = data.loc.to(device)\n",
    "        vel = data.vel.to(device)\n",
    "        x = data.x.to(device)\n",
    "        edges = data.edge_index.to(device)\n",
    "        edge_attr = data.edge_attr.to(device)\n",
    "        loc_end = data.next_loc.to(device)\n",
    "        vel_end = data.next_vel.to(device)\n",
    "\n",
    "        # loc, vel, x, edges, edge_attr, loc_end = data # TODO posso farlgli passare anche gli edges\n",
    "\n",
    "        # edges = loader.dataset.get_edges(batch_size, n_nodes) # TODO questa va tolta\n",
    "        edges = [edges[0].to(device), edges[1].to(device)]\n",
    "\n",
    "        \"\"\"\n",
    "            Batch idx:  0\n",
    "            Data: ...\n",
    "            Batch size:  100\n",
    "            Number of nodes:  5\n",
    "            Data:  torch.Size([100, 5, 3])\n",
    "            Data:  torch.Size([500, 3])\n",
    "            train epoch 18 avg loss: 0.01948\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if time_exp:\n",
    "        #     torch.cuda.synchronize()\n",
    "        #     t1 = time.time()\n",
    "        \n",
    "\n",
    "        # nodes = torch.sqrt(torch.sum(vel ** 2, dim=1)).unsqueeze(1).detach()\n",
    "        # rows, cols = edges\n",
    "        # loc_dist = torch.sum((loc[rows] - loc[cols])**2, 1).unsqueeze(1)  # relative distances among locations\n",
    "        # edge_attr = torch.cat([edge_attr, loc_dist], 1).detach()  # concatenate all edge properties\n",
    "\n",
    "        # loc_pred = model(nodes, loc.detach(), edges, vel, edge_attr)\n",
    "        # loc_pred = model(x, loc.detach(), edges, vel, edge_attr)\n",
    "        loc_pred, vel_pred = model(x, loc.detach(), edges, vel, edge_attr)\n",
    "\n",
    "\n",
    "        # if time_exp:\n",
    "        #     torch.cuda.synchronize()\n",
    "        #     t2 = time.time()\n",
    "        #     time_exp_dic['time'] += t2 - t1\n",
    "        #     time_exp_dic['counter'] += 1\n",
    "\n",
    "            # print(\"Forward average time: %.6f\" % (time_exp_dic['time'] / time_exp_dic['counter']))\n",
    "        \n",
    "        # loss = loss_mse(loc_pred, loc_end)\n",
    "        loss = loss_mse(vel_pred, vel_end)\n",
    "\n",
    "        if backprop:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        res['loss'] += loss.item()*batch_size\n",
    "        res['counter'] += batch_size\n",
    "\n",
    "        # if batch_idx % log_interval == 0 and (model == \"se3_transformer\" or model == \"tfn\"):\n",
    "        #     print('===> {} Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(loader.dataset.partition,\n",
    "        #         epoch, batch_idx * batch_size, len(loader.dataset),\n",
    "        #         100. * batch_idx / len(loader),\n",
    "        #         loss.item()))\n",
    "\n",
    "    if not backprop:\n",
    "        prefix = \"==> \"\n",
    "    else:\n",
    "        prefix = \"\"\n",
    "    print('%s epoch %d avg loss: %.5f' % (prefix, epoch, res['loss'] / res['counter']))\n",
    "    # print('%s epoch %d avg loss: %.5f' % (prefix+loader.dataset.partition, epoch, res['loss'] / res['counter']))\n",
    "\n",
    "    return res['loss'] / res['counter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_procedure(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 avg loss: 0.33256\n",
      "==>  epoch 0 avg loss: 0.31569\n",
      "==>  epoch 0 avg loss: 0.30969\n",
      "*** Best Val Loss: 0.31569 \t Best Test Loss: 0.30969 \t Best epoch 0\n",
      " epoch 1 avg loss: 0.31748\n",
      " epoch 2 avg loss: 0.31677\n",
      " epoch 3 avg loss: 0.31584\n",
      " epoch 4 avg loss: 0.31450\n",
      " epoch 5 avg loss: 0.31357\n",
      "==>  epoch 5 avg loss: 0.31332\n",
      "==>  epoch 5 avg loss: 0.30697\n",
      "*** Best Val Loss: 0.31332 \t Best Test Loss: 0.30697 \t Best epoch 5\n",
      " epoch 6 avg loss: 0.31268\n",
      " epoch 7 avg loss: 0.31195\n",
      " epoch 8 avg loss: 0.31087\n",
      " epoch 9 avg loss: 0.30980\n",
      " epoch 10 avg loss: 0.30847\n",
      "==>  epoch 10 avg loss: 0.30799\n",
      "==>  epoch 10 avg loss: 0.30135\n",
      "*** Best Val Loss: 0.30799 \t Best Test Loss: 0.30135 \t Best epoch 10\n",
      " epoch 11 avg loss: 0.30734\n",
      " epoch 12 avg loss: 0.30610\n",
      " epoch 13 avg loss: 0.30480\n",
      " epoch 14 avg loss: 0.30370\n",
      " epoch 15 avg loss: 0.30255\n",
      "==>  epoch 15 avg loss: 0.30354\n",
      "==>  epoch 15 avg loss: 0.29833\n",
      "*** Best Val Loss: 0.30354 \t Best Test Loss: 0.29833 \t Best epoch 15\n",
      " epoch 16 avg loss: 0.30006\n",
      " epoch 17 avg loss: 0.29820\n",
      " epoch 18 avg loss: 0.29706\n",
      " epoch 19 avg loss: 0.29350\n",
      " epoch 20 avg loss: 0.29111\n",
      "==>  epoch 20 avg loss: 0.29188\n",
      "==>  epoch 20 avg loss: 0.28730\n",
      "*** Best Val Loss: 0.29188 \t Best Test Loss: 0.28730 \t Best epoch 20\n",
      " epoch 21 avg loss: 0.28675\n",
      " epoch 22 avg loss: 0.28320\n",
      " epoch 23 avg loss: 0.28056\n",
      " epoch 24 avg loss: 0.27540\n",
      " epoch 25 avg loss: 0.27246\n",
      "==>  epoch 25 avg loss: 0.28300\n",
      "==>  epoch 25 avg loss: 0.28030\n",
      "*** Best Val Loss: 0.28300 \t Best Test Loss: 0.28030 \t Best epoch 25\n",
      " epoch 26 avg loss: 0.27041\n",
      " epoch 27 avg loss: 0.26539\n",
      " epoch 28 avg loss: 0.25831\n",
      " epoch 29 avg loss: 0.25366\n",
      " epoch 30 avg loss: 0.24763\n",
      "==>  epoch 30 avg loss: 0.26886\n",
      "==>  epoch 30 avg loss: 0.26451\n",
      "*** Best Val Loss: 0.26886 \t Best Test Loss: 0.26451 \t Best epoch 30\n",
      " epoch 31 avg loss: 0.24299\n",
      " epoch 32 avg loss: 0.23814\n",
      " epoch 33 avg loss: 0.23207\n",
      " epoch 34 avg loss: 0.22624\n",
      " epoch 35 avg loss: 0.22229\n",
      "==>  epoch 35 avg loss: 0.25049\n",
      "==>  epoch 35 avg loss: 0.24623\n",
      "*** Best Val Loss: 0.25049 \t Best Test Loss: 0.24623 \t Best epoch 35\n",
      " epoch 36 avg loss: 0.21607\n",
      " epoch 37 avg loss: 0.21274\n",
      " epoch 38 avg loss: 0.20672\n",
      " epoch 39 avg loss: 0.20131\n",
      " epoch 40 avg loss: 0.19676\n",
      "==>  epoch 40 avg loss: 0.23477\n",
      "==>  epoch 40 avg loss: 0.23273\n",
      "*** Best Val Loss: 0.23477 \t Best Test Loss: 0.23273 \t Best epoch 40\n",
      " epoch 41 avg loss: 0.19189\n",
      " epoch 42 avg loss: 0.18680\n",
      " epoch 43 avg loss: 0.18272\n",
      " epoch 44 avg loss: 0.17837\n",
      " epoch 45 avg loss: 0.17441\n",
      "==>  epoch 45 avg loss: 0.22583\n",
      "==>  epoch 45 avg loss: 0.22228\n",
      "*** Best Val Loss: 0.22583 \t Best Test Loss: 0.22228 \t Best epoch 45\n",
      " epoch 46 avg loss: 0.16960\n",
      " epoch 47 avg loss: 0.16560\n",
      " epoch 48 avg loss: 0.16186\n",
      " epoch 49 avg loss: 0.15765\n",
      " epoch 50 avg loss: 0.15359\n",
      "==>  epoch 50 avg loss: 0.21494\n",
      "==>  epoch 50 avg loss: 0.21126\n",
      "*** Best Val Loss: 0.21494 \t Best Test Loss: 0.21126 \t Best epoch 50\n",
      " epoch 51 avg loss: 0.15009\n",
      " epoch 52 avg loss: 0.14695\n",
      " epoch 53 avg loss: 0.14352\n",
      " epoch 54 avg loss: 0.14071\n",
      " epoch 55 avg loss: 0.13716\n",
      "==>  epoch 55 avg loss: 0.21321\n",
      "==>  epoch 55 avg loss: 0.20909\n",
      "*** Best Val Loss: 0.21321 \t Best Test Loss: 0.20909 \t Best epoch 55\n",
      " epoch 56 avg loss: 0.13393\n",
      " epoch 57 avg loss: 0.13286\n",
      " epoch 58 avg loss: 0.12819\n",
      " epoch 59 avg loss: 0.12654\n",
      " epoch 60 avg loss: 0.12386\n",
      "==>  epoch 60 avg loss: 0.20799\n",
      "==>  epoch 60 avg loss: 0.20567\n",
      "*** Best Val Loss: 0.20799 \t Best Test Loss: 0.20567 \t Best epoch 60\n",
      " epoch 61 avg loss: 0.12095\n",
      " epoch 62 avg loss: 0.11810\n",
      " epoch 63 avg loss: 0.11673\n",
      " epoch 64 avg loss: 0.11365\n",
      " epoch 65 avg loss: 0.11205\n",
      "==>  epoch 65 avg loss: 0.20534\n",
      "==>  epoch 65 avg loss: 0.20395\n",
      "*** Best Val Loss: 0.20534 \t Best Test Loss: 0.20395 \t Best epoch 65\n",
      " epoch 66 avg loss: 0.10987\n",
      " epoch 67 avg loss: 0.10784\n",
      " epoch 68 avg loss: 0.10599\n",
      " epoch 69 avg loss: 0.10383\n",
      " epoch 70 avg loss: 0.10191\n",
      "==>  epoch 70 avg loss: 0.20441\n",
      "==>  epoch 70 avg loss: 0.20108\n",
      "*** Best Val Loss: 0.20441 \t Best Test Loss: 0.20108 \t Best epoch 70\n",
      " epoch 71 avg loss: 0.10056\n",
      " epoch 72 avg loss: 0.09851\n",
      " epoch 73 avg loss: 0.09734\n",
      " epoch 74 avg loss: 0.09493\n",
      " epoch 75 avg loss: 0.09388\n",
      "==>  epoch 75 avg loss: 0.20272\n",
      "==>  epoch 75 avg loss: 0.19973\n",
      "*** Best Val Loss: 0.20272 \t Best Test Loss: 0.19973 \t Best epoch 75\n",
      " epoch 76 avg loss: 0.09189\n",
      " epoch 77 avg loss: 0.09160\n",
      " epoch 78 avg loss: 0.08995\n",
      " epoch 79 avg loss: 0.08841\n",
      " epoch 80 avg loss: 0.08740\n",
      "==>  epoch 80 avg loss: 0.20075\n",
      "==>  epoch 80 avg loss: 0.19847\n",
      "*** Best Val Loss: 0.20075 \t Best Test Loss: 0.19847 \t Best epoch 80\n",
      " epoch 81 avg loss: 0.08610\n",
      " epoch 82 avg loss: 0.08410\n",
      " epoch 83 avg loss: 0.08345\n",
      " epoch 84 avg loss: 0.08170\n",
      " epoch 85 avg loss: 0.08079\n",
      "==>  epoch 85 avg loss: 0.20287\n",
      "==>  epoch 85 avg loss: 0.20030\n",
      "*** Best Val Loss: 0.20075 \t Best Test Loss: 0.19847 \t Best epoch 80\n",
      " epoch 86 avg loss: 0.07966\n",
      " epoch 87 avg loss: 0.07963\n",
      " epoch 88 avg loss: 0.07821\n",
      " epoch 89 avg loss: 0.07699\n",
      " epoch 90 avg loss: 0.07565\n",
      "==>  epoch 90 avg loss: 0.19954\n",
      "==>  epoch 90 avg loss: 0.19637\n",
      "*** Best Val Loss: 0.19954 \t Best Test Loss: 0.19637 \t Best epoch 90\n",
      " epoch 91 avg loss: 0.07536\n",
      " epoch 92 avg loss: 0.07410\n",
      " epoch 93 avg loss: 0.07383\n",
      " epoch 94 avg loss: 0.07283\n",
      " epoch 95 avg loss: 0.07165\n",
      "==>  epoch 95 avg loss: 0.20312\n",
      "==>  epoch 95 avg loss: 0.19786\n",
      "*** Best Val Loss: 0.19954 \t Best Test Loss: 0.19637 \t Best epoch 90\n",
      " epoch 96 avg loss: 0.07143\n",
      " epoch 97 avg loss: 0.07019\n",
      " epoch 98 avg loss: 0.07000\n",
      " epoch 99 avg loss: 0.06975\n",
      " epoch 100 avg loss: 0.06764\n",
      "==>  epoch 100 avg loss: 0.19941\n",
      "==>  epoch 100 avg loss: 0.19656\n",
      "*** Best Val Loss: 0.19941 \t Best Test Loss: 0.19656 \t Best epoch 100\n",
      " epoch 101 avg loss: 0.06746\n",
      " epoch 102 avg loss: 0.06710\n",
      " epoch 103 avg loss: 0.06626\n",
      " epoch 104 avg loss: 0.06529\n",
      " epoch 105 avg loss: 0.06510\n",
      "==>  epoch 105 avg loss: 0.20208\n",
      "==>  epoch 105 avg loss: 0.19654\n",
      "*** Best Val Loss: 0.19941 \t Best Test Loss: 0.19656 \t Best epoch 100\n",
      " epoch 106 avg loss: 0.06426\n",
      " epoch 107 avg loss: 0.06360\n",
      " epoch 108 avg loss: 0.06270\n",
      " epoch 109 avg loss: 0.06319\n",
      " epoch 110 avg loss: 0.06182\n",
      "==>  epoch 110 avg loss: 0.20107\n",
      "==>  epoch 110 avg loss: 0.19690\n",
      "*** Best Val Loss: 0.19941 \t Best Test Loss: 0.19656 \t Best epoch 100\n",
      " epoch 111 avg loss: 0.06104\n",
      " epoch 112 avg loss: 0.06115\n",
      " epoch 113 avg loss: 0.06047\n",
      " epoch 114 avg loss: 0.06000\n",
      " epoch 115 avg loss: 0.05876\n",
      "==>  epoch 115 avg loss: 0.19881\n",
      "==>  epoch 115 avg loss: 0.19561\n",
      "*** Best Val Loss: 0.19881 \t Best Test Loss: 0.19561 \t Best epoch 115\n",
      " epoch 116 avg loss: 0.05901\n",
      " epoch 117 avg loss: 0.05908\n",
      " epoch 118 avg loss: 0.05765\n",
      " epoch 119 avg loss: 0.05756\n",
      " epoch 120 avg loss: 0.05761\n",
      "==>  epoch 120 avg loss: 0.19920\n",
      "==>  epoch 120 avg loss: 0.19459\n",
      "*** Best Val Loss: 0.19881 \t Best Test Loss: 0.19561 \t Best epoch 115\n",
      " epoch 121 avg loss: 0.05605\n",
      " epoch 122 avg loss: 0.05547\n",
      " epoch 123 avg loss: 0.05569\n",
      " epoch 124 avg loss: 0.05541\n",
      " epoch 125 avg loss: 0.05501\n",
      "==>  epoch 125 avg loss: 0.20183\n",
      "==>  epoch 125 avg loss: 0.19661\n",
      "*** Best Val Loss: 0.19881 \t Best Test Loss: 0.19561 \t Best epoch 115\n",
      " epoch 126 avg loss: 0.05539\n",
      " epoch 127 avg loss: 0.05467\n",
      " epoch 128 avg loss: 0.05373\n",
      " epoch 129 avg loss: 0.05412\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# dummy_data = dataset[0]\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# print(summary(model, dummy_data.x, dummy_data.loc, dummy_data.edge_index, dummy_data.vel, dummy_data.edge_attr))\u001b[39;00m\n\u001b[0;32m     46\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtraining_procedure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m, in \u001b[0;36mtraining_procedure\u001b[1;34m(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf)\u001b[0m\n\u001b[0;32m      5\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs):\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m test_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m#train(epoch, loader_train, backprop=False)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m train(model, optimizer, epoch, loader_val, backprop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[46], line 78\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, epoch, loader, backprop)\u001b[0m\n\u001b[0;32m     75\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_mse(vel_pred, vel_end)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backprop:\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     81\u001b[0m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mbatch_size\n",
      "File \u001b[1;32md:\\Projects\\Deep Learning\\DeepLearningAssignment2\\.venv\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\Deep Learning\\DeepLearningAssignment2\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\Deep Learning\\DeepLearningAssignment2\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Smaller batch size\n",
    "model_name = 'egnn_task2_2'\n",
    "batch_size = 4\n",
    "epochs = 300\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "log_interval = 1\n",
    "test_interval = 5\n",
    "outf = './logs'\n",
    "lr = 5e-4\n",
    "nf = 64\n",
    "attention = 0\n",
    "n_layers = 4\n",
    "degree = 2\n",
    "max_training_samples = 3000\n",
    "dataset = \"nbody_small\"\n",
    "sweep_training = 0\n",
    "time_exp = 0\n",
    "weight_decay = 1e-12\n",
    "div = 1\n",
    "norm_diff = False\n",
    "tanh = False\n",
    "\n",
    "time_exp_dic = {'time': 0, 'counter': 0}\n",
    "\n",
    "# Create log directory\n",
    "try:\n",
    "    os.makedirs(outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(outf + \"/\" + model_name)\n",
    "except OSError:\n",
    "    pass\n",
    "# __________________________________________________________________________________________________ #\n",
    "\n",
    "model = EGNN_vel(in_node_nf=1, in_edge_nf=2, hidden_nf=nf, device=device, n_layers=n_layers, recurrent=True, norm_diff=norm_diff, tanh=tanh).to(device)\n",
    "\n",
    "# dummy_data = dataset[0]\n",
    "# print(summary(model, dummy_data.x, dummy_data.loc, dummy_data.edge_index, dummy_data.vel, dummy_data.edge_attr))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "training_procedure(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller model\n",
    "model_name = 'egnn_task2_3'\n",
    "batch_size = 16\n",
    "epochs = 300\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "log_interval = 1\n",
    "test_interval = 5\n",
    "outf = './logs'\n",
    "lr = 5e-4\n",
    "nf = 32\n",
    "attention = 0\n",
    "n_layers = 4\n",
    "degree = 2\n",
    "max_training_samples = 3000\n",
    "dataset = \"nbody_small\"\n",
    "sweep_training = 0\n",
    "time_exp = 0\n",
    "weight_decay = 1e-12\n",
    "div = 1\n",
    "norm_diff = False\n",
    "tanh = False\n",
    "\n",
    "time_exp_dic = {'time': 0, 'counter': 0}\n",
    "\n",
    "# Create log directory\n",
    "try:\n",
    "    os.makedirs(outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(outf + \"/\" + model_name)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# __________________________________________________________________________________________________ #\n",
    "\n",
    "model = EGNN_vel(in_node_nf=1, in_edge_nf=2, hidden_nf=nf, device=device, n_layers=n_layers, recurrent=True, norm_diff=norm_diff, tanh=tanh).to(device)\n",
    "\n",
    "# dummy_data = dataset[0]\n",
    "# print(summary(model, dummy_data.x, dummy_data.loc, dummy_data.edge_index, dummy_data.vel, dummy_data.edge_attr))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "training_procedure(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowe lr\n",
    "model_name = 'egnn_task2_4'\n",
    "batch_size = 16\n",
    "epochs = 300\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "log_interval = 1\n",
    "test_interval = 5\n",
    "outf = './logs'\n",
    "lr = 1e-4\n",
    "nf = 64\n",
    "attention = 0\n",
    "n_layers = 4\n",
    "degree = 2\n",
    "max_training_samples = 3000\n",
    "dataset = \"nbody_small\"\n",
    "sweep_training = 0\n",
    "time_exp = 0\n",
    "weight_decay = 1e-12\n",
    "div = 1\n",
    "norm_diff = False\n",
    "tanh = False\n",
    "\n",
    "time_exp_dic = {'time': 0, 'counter': 0}\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Create log directory\n",
    "try:\n",
    "    os.makedirs(outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(outf + \"/\" + model_name)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# __________________________________________________________________________________________________ #\n",
    "\n",
    "model = EGNN_vel(in_node_nf=1, in_edge_nf=2, hidden_nf=nf, device=device, n_layers=n_layers, recurrent=True, norm_diff=norm_diff, tanh=tanh).to(device)\n",
    "\n",
    "dummy_data = dataset_train[0]\n",
    "print(summary(model, dummy_data.x, dummy_data.loc, dummy_data.edge_index, dummy_data.vel, dummy_data.edge_attr))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "training_procedure(model, optimizer, loader_train, loader_val, loader_test, epochs, test_interval, model_name, outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new model EGNN_vel and load the best model from the previous training\n",
    "model = EGNN_vel(in_node_nf=1, in_edge_nf=2, hidden_nf=nf, device=device, n_layers=n_layers, recurrent=True, norm_diff=norm_diff, tanh=tanh).to(device)\n",
    "model.load_state_dict(torch.load(outf + \"/\" + model_name + f\"/{model_name}.pth\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = loader_train.dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0657,  0.7710],\n",
      "        [ 0.2821,  0.0322],\n",
      "        [ 0.6056,  0.2085],\n",
      "        [ 0.2146,  0.0398],\n",
      "        [-0.1661, -0.5974]])\n",
      "tensor([[15.3184, 17.6959],\n",
      "        [12.0323, 18.8535],\n",
      "        [12.5798, 14.2019],\n",
      "        [12.1333,  7.2062],\n",
      "        [ 1.3248,  5.5971]])\n"
     ]
    }
   ],
   "source": [
    "print(graph.next_vel)\n",
    "print(graph.next_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vel_pred: tensor([[ 0.0349,  0.5218],\n",
      "        [ 0.2865,  0.0644],\n",
      "        [ 0.9613,  0.3073],\n",
      "        [ 0.1873,  0.1281],\n",
      "        [-0.0262, -0.5257]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loc_pred: tensor([[12.4386, 13.7175],\n",
      "        [10.1018, 15.0359],\n",
      "        [ 9.6235, 12.3083],\n",
      "        [10.9564, 10.4238],\n",
      "        [ 9.7249, 12.3771]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = graph.x.to(device)\n",
    "loc = graph.loc.to(device)\n",
    "vel = graph.vel.to(device)\n",
    "edges = graph.edge_index.to(device)\n",
    "edge_attr = graph.edge_attr.to(device)\n",
    "edges = [edges[0].to(device), edges[1].to(device)]\n",
    "\n",
    "loc_pred, vel_pred = model(x, loc.detach(), edges, vel, edge_attr)\n",
    "print(f\"vel_pred: {vel_pred}\")\n",
    "print(f\"loc_pred: {loc_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
